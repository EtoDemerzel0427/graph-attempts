{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import tensorflow as tf\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _LINE(object):\n",
    "\n",
    "    def __init__(self, graph, rep_size=128, batch_size=1000, negative_ratio=5, order=3, cnt_u=15000, cnt_v=2529):\n",
    "        self.cur_epoch = 0\n",
    "        self.order = order\n",
    "        self.g = graph\n",
    "        self.node_size = graph.G.number_of_nodes()\n",
    "        self.rep_size = rep_size\n",
    "        self.batch_size = batch_size\n",
    "        self.negative_ratio = negative_ratio\n",
    "\n",
    "        self.gen_sampling_table()\n",
    "\n",
    "    \n",
    "        self.sess = tf.Session()\n",
    "        cur_seed = random.getrandbits(32)\n",
    "        initializer = tf.contrib.layers.xavier_initializer(\n",
    "            uniform=False, seed=cur_seed)\n",
    "        with tf.variable_scope(\"model\", reuse=None, initializer=initializer):\n",
    "            self.build_graph()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    def build_graph(self):\n",
    "        self.h = tf.placeholder(tf.int32, [None])\n",
    "        self.t = tf.placeholder(tf.int32, [None])\n",
    "        self.sign = tf.placeholder(tf.float32, [None])\n",
    "\n",
    "        cur_seed = random.getrandbits(32)\n",
    "        self.embeddings = tf.get_variable(name=\"embeddings\"+str(self.order), shape=[\n",
    "                                          self.node_size, self.rep_size], initializer=tf.contrib.layers.xavier_initializer(uniform=False, seed=cur_seed))\n",
    "        self.context_embeddings = tf.get_variable(name=\"context_embeddings\"+str(self.order), shape=[\n",
    "                                                  self.node_size, self.rep_size], initializer=tf.contrib.layers.xavier_initializer(uniform=False, seed=cur_seed))\n",
    "\n",
    "        self.h_e = tf.nn.embedding_lookup(self.embeddings, self.h)\n",
    "        self.t_e = tf.nn.embedding_lookup(self.embeddings, self.t)\n",
    "        self.t_e_context = tf.nn.embedding_lookup(\n",
    "            self.context_embeddings, self.t)\n",
    "        self.second_loss = -tf.reduce_mean(tf.log_sigmoid(\n",
    "            self.sign*tf.reduce_sum(tf.multiply(self.h_e, self.t_e_context), axis=1)))\n",
    "        self.first_loss = -tf.reduce_mean(tf.log_sigmoid(\n",
    "            self.sign*tf.reduce_sum(tf.multiply(self.h_e, self.t_e), axis=1)))\n",
    "        if self.order == 1:\n",
    "            self.loss = self.first_loss\n",
    "        else:\n",
    "            self.loss = self.second_loss\n",
    "        optimizer = tf.train.AdamOptimizer(0.001)\n",
    "        self.train_op = optimizer.minimize(self.loss)\n",
    "\n",
    "    def train_one_epoch(self):\n",
    "        sum_loss = 0.0\n",
    "        batches1 = self.batch_iter_heter()\n",
    "        batches2 = self.batch_iter_homo(isM=True)\n",
    "        batches3 = self.batch_iter_homo(isM=False)\n",
    "        batch_id = 0\n",
    "        for batch in chain(batches1, batches2, batches3):\n",
    "            h, t, sign = batch\n",
    "            feed_dict = {\n",
    "                self.h: h,\n",
    "                self.t: t,\n",
    "                self.sign: sign,\n",
    "            }\n",
    "            _, cur_loss = self.sess.run([self.train_op, self.loss], feed_dict)\n",
    "            sum_loss += cur_loss\n",
    "            batch_id += 1\n",
    "        print('epoch:{} sum of loss:{!s}'.format(self.cur_epoch, sum_loss))\n",
    "        self.cur_epoch += 1\n",
    "\n",
    "    def batch_iter_heter(self):\n",
    "        look_up = self.g.look_up_dict\n",
    "\n",
    "        table_size = 1e8\n",
    "        numNodes = self.node_size\n",
    "\n",
    "        edges = [(look_up[x[0]], look_up[x[1]]) for x in self.g.G.edges()]\n",
    "\n",
    "        data_size = self.g.G.number_of_edges()\n",
    "        edge_set = set([x[0]*numNodes+x[1] for x in edges])\n",
    "        shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "\n",
    "        # positive or negative mod\n",
    "        mod = 0\n",
    "        mod_size = 1 + self.negative_ratio\n",
    "        h = []\n",
    "        t = []\n",
    "        sign = 0\n",
    "\n",
    "        start_index = 0\n",
    "        end_index = min(start_index+self.batch_size, data_size)\n",
    "        while start_index < data_size:\n",
    "            if mod == 0:\n",
    "                sign = 1.\n",
    "                h = []\n",
    "                t = []\n",
    "                for i in range(start_index, end_index):\n",
    "                    if not random.random() < self.edge_prob[shuffle_indices[i]]:\n",
    "                        shuffle_indices[i] = self.edge_alias[shuffle_indices[i]]\n",
    "                    cur_h = edges[shuffle_indices[i]][0]\n",
    "                    cur_t = edges[shuffle_indices[i]][1]\n",
    "                    h.append(cur_h)\n",
    "                    t.append(cur_t)\n",
    "            else:\n",
    "                sign = -1.\n",
    "                t = []\n",
    "                for i in range(len(h)):\n",
    "                    if self.g.look_back_list[h[i]][0] == 'u':\n",
    "                        t.append(self.item_table[random.randint(0, table_size-1)])\n",
    "                    else:\n",
    "                        t.append(self.user_table[random.randint(0, table_size-1)])\n",
    "\n",
    "            yield h, t, [sign]\n",
    "            mod += 1\n",
    "            mod %= mod_size\n",
    "            if mod == 0:\n",
    "                start_index = end_index\n",
    "                end_index = min(start_index+self.batch_size, data_size)\n",
    "                \n",
    "    def batch_iter_homo(self, isM = True):\n",
    "        look_up = self.g.look_up_dict\n",
    "\n",
    "        table_size = 1e8\n",
    "        if isM:\n",
    "            numNodes = self.g.M.number_of_nodes()\n",
    "            edges = [(x[0], x[1]) for x in self.g.M.edges]\n",
    "            data_size = self.g.M.number_of_edges()\n",
    "            prob = self.prob_M\n",
    "            alias = self.alias_M\n",
    "        else:\n",
    "            numNodes = self.g.N.number_of_nodes()\n",
    "            bias = self.g.M.number_of_nodes()\n",
    "            edges = [(x[0] + bias, x[1] + bias) for x in self.g.N.edges()]\n",
    "            data_size = self.g.N.number_of_edges()\n",
    "            prob = self.prob_N\n",
    "            alias = self.alias_N\n",
    "        \n",
    "\n",
    "        shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "\n",
    "        # positive or negative mod\n",
    "        mod = 0\n",
    "        mod_size = 1 + self.negative_ratio\n",
    "        h = []\n",
    "        t = []\n",
    "        sign = 0\n",
    "\n",
    "        start_index = 0\n",
    "        end_index = min(start_index+self.batch_size, data_size)\n",
    "        while start_index < data_size:\n",
    "            if mod == 0:\n",
    "                sign = 1.\n",
    "                h = []\n",
    "                t = []\n",
    "                for i in range(start_index, end_index):\n",
    "                    if not random.random() < prob[shuffle_indices[i]]:\n",
    "                        shuffle_indices[i] = alias[shuffle_indices[i]]\n",
    "                    cur_h = edges[shuffle_indices[i]][0]\n",
    "                    cur_t = edges[shuffle_indices[i]][1]\n",
    "                    h.append(cur_h)\n",
    "                    t.append(cur_t)\n",
    "            else:\n",
    "                sign = -1.\n",
    "                t = []\n",
    "                for i in range(len(h)):\n",
    "                    if isM:\n",
    "                        t.append(self.uu_table[random.randint(0, table_size-1)])\n",
    "                    else:\n",
    "                        t.append(self.ii_table[random.randint(0, table_size-1)])\n",
    "\n",
    "            yield h, t, [sign]\n",
    "            mod += 1\n",
    "            mod %= mod_size\n",
    "            if mod == 0:\n",
    "                start_index = end_index\n",
    "                end_index = min(start_index+self.batch_size, data_size)\n",
    "\n",
    "    def gen_sampling_table(self):\n",
    "        table_size = 1e8\n",
    "        power = 0.75\n",
    "        numNodes = self.node_size\n",
    "\n",
    "        print(\"Pre-procesing for non-uniform negative sampling!\")\n",
    "        pos_u = [i for i in range(numNodes) if self.g.look_back_list[i][0]=='u']\n",
    "        pos_v = [i for i in range(numNodes) if self.g.look_back_list[i][0]=='i']\n",
    "        degrees = np.array(self.g.G.degree())[:,1]\n",
    "        degree_u = np.array(degrees[pos_u], dtype=np.int32)\n",
    "        degree_v = np.array(degrees[pos_v], dtype=np.int32)\n",
    "        \n",
    "        self.user_table = self.get_distribution(degree_u)\n",
    "        self.item_table = self.get_distribution(degree_v)\n",
    "        \n",
    "        degree_homo_u = np.array(self.g.M.degree())[:,1]\n",
    "        degree_homo_v = np.array(self.g.N.degree())[:,1]\n",
    "        \n",
    "        self.uu_table = self.get_distribution(degree_homo_u)\n",
    "        self.ii_table = self.get_distribution(degree_homo_v)\n",
    "\n",
    "        \n",
    "        self.edge_prob, self.edge_alias = self.alias_method(self.g.G)\n",
    "        \n",
    "        self.prob_M, self.alias_M = self.alias_method(self.g.M)\n",
    "        \n",
    "        self.prob_N, self.alias_N = self.alias_method(self.g.N)\n",
    "        \n",
    "    def get_distribution(self, degree_vec):\n",
    "        table_size = 1e8\n",
    "        power = 0.75\n",
    "        \n",
    "        x = np.power(degree_vec, power)\n",
    "        norm = np.sum(x)\n",
    "        print('The norm is', norm)\n",
    "        \n",
    "        table = np.zeros(int(table_size), dtype = np.uint32)\n",
    "        \n",
    "        \n",
    "        p = 0\n",
    "        i = 0\n",
    "        num = len(degree_vec)\n",
    "        for j in range(num):\n",
    "            p = np.sum(x[0: j + 1])/norm\n",
    "            # p += np.power(node_degree[j], power) / norm\n",
    "            while i < table_size and (i / table_size) < p:\n",
    "                table[i] = j\n",
    "                i += 1\n",
    "                \n",
    "        return table\n",
    "    \n",
    "    def alias_method(self, G):\n",
    "        data_size = G.number_of_edges()\n",
    "        edge_alias = np.zeros(data_size, dtype=np.int32)\n",
    "        edge_prob = np.zeros(data_size, dtype=np.float32)\n",
    "        large_block = np.zeros(data_size, dtype=np.int32)\n",
    "        small_block = np.zeros(data_size, dtype=np.int32)\n",
    "\n",
    "        total_sum = sum([G[edge[0]][edge[1]][\"weight\"]\n",
    "                         for edge in G.edges()])\n",
    "        norm_prob = [G[edge[0]][edge[1]][\"weight\"] *\n",
    "                     data_size/total_sum for edge in G.edges()]\n",
    "        \n",
    "        num_small_block = 0\n",
    "        num_large_block = 0\n",
    "        cur_small_block = 0\n",
    "        cur_large_block = 0\n",
    "        for k in range(data_size-1, -1, -1):\n",
    "            if norm_prob[k] < 1:\n",
    "                small_block[num_small_block] = k\n",
    "                num_small_block += 1\n",
    "            else:\n",
    "                large_block[num_large_block] = k\n",
    "                num_large_block += 1\n",
    "        while num_small_block and num_large_block:\n",
    "            num_small_block -= 1\n",
    "            cur_small_block = small_block[num_small_block]\n",
    "            num_large_block -= 1\n",
    "            cur_large_block = large_block[num_large_block]\n",
    "            \n",
    "            edge_prob[cur_small_block] = norm_prob[cur_small_block]\n",
    "            edge_alias[cur_small_block] = cur_large_block\n",
    "            norm_prob[cur_large_block] = norm_prob[cur_large_block] + \\\n",
    "                norm_prob[cur_small_block] - 1\n",
    "            if norm_prob[cur_large_block] < 1:\n",
    "                small_block[num_small_block] = cur_large_block\n",
    "                num_small_block += 1\n",
    "            else:\n",
    "                large_block[num_large_block] = cur_large_block\n",
    "                num_large_block += 1\n",
    "\n",
    "        while num_large_block:\n",
    "            num_large_block -= 1\n",
    "            edge_prob[large_block[num_large_block]] = 1\n",
    "        while num_small_block:\n",
    "            num_small_block -= 1\n",
    "            edge_prob[small_block[num_small_block]] = 1\n",
    "            \n",
    "        return edge_prob, edge_alias\n",
    "\n",
    "    def get_embeddings(self):\n",
    "        vectors = {}\n",
    "        embeddings = self.embeddings.eval(session=self.sess)\n",
    "        # embeddings = self.sess.run(tf.nn.l2_normalize(self.embeddings.eval(session=self.sess), 1))\n",
    "        look_back = self.g.look_back_list\n",
    "        for i, embedding in enumerate(embeddings):\n",
    "            vectors[look_back[i]] = embedding\n",
    "        return vectors\n",
    "\n",
    "\n",
    "class LINE(object):\n",
    "\n",
    "    def __init__(self, graph, rep_size=128, batch_size=1000, epoch=10, negative_ratio=5, order=3, cnt_u=15000, cnt_v=2529, label_file=None, clf_ratio=0.5, auto_save=True):\n",
    "        self.rep_size = rep_size\n",
    "        self.order = order\n",
    "        self.best_result = 0\n",
    "        self.vectors = {}\n",
    "        if order == 3:\n",
    "            self.model1 = _LINE(graph, rep_size/2, batch_size,\n",
    "                                negative_ratio, order=1)\n",
    "            self.model2 = _LINE(graph, rep_size/2, batch_size,\n",
    "                                negative_ratio, order=2)\n",
    "            for i in range(epoch):\n",
    "                self.model1.train_one_epoch()\n",
    "                self.model2.train_one_epoch()\n",
    "                if label_file:\n",
    "                    self.get_embeddings()\n",
    "                    X, Y = read_node_label(label_file)\n",
    "                    print(\"Training classifier using {:.2f}% nodes...\".format(\n",
    "                        clf_ratio*100))\n",
    "                    clf = Classifier(vectors=self.vectors,\n",
    "                                     clf=LogisticRegression())\n",
    "                    result = clf.split_train_evaluate(X, Y, clf_ratio)\n",
    "\n",
    "                    if result['macro'] > self.best_result:\n",
    "                        self.best_result = result['macro']\n",
    "                        if auto_save:\n",
    "                            self.best_vector = self.vectors\n",
    "\n",
    "        else:\n",
    "            self.model = _LINE(graph, rep_size, batch_size,\n",
    "                               negative_ratio, order=self.order)\n",
    "            for i in range(epoch):\n",
    "                self.model.train_one_epoch()\n",
    "                if label_file:\n",
    "                    self.get_embeddings()\n",
    "                    X, Y = read_node_label(label_file)\n",
    "                    print(\"Training classifier using {:.2f}% nodes...\".format(\n",
    "                        clf_ratio*100))\n",
    "                    clf = Classifier(vectors=self.vectors,\n",
    "                                     clf=LogisticRegression())\n",
    "                    result = clf.split_train_evaluate(X, Y, clf_ratio)\n",
    "\n",
    "                    if result['macro'] > self.best_result:\n",
    "                        self.best_result = result['macro']\n",
    "                        if auto_save:\n",
    "                            self.best_vector = self.vectors\n",
    "\n",
    "        self.get_embeddings()\n",
    "        if auto_save and label_file:\n",
    "            self.vectors = self.best_vector\n",
    "\n",
    "    def get_embeddings(self):\n",
    "        self.last_vectors = self.vectors\n",
    "        self.vectors = {}\n",
    "        if self.order == 3:\n",
    "            vectors1 = self.model1.get_embeddings()\n",
    "            vectors2 = self.model2.get_embeddings()\n",
    "            for node in vectors1.keys():\n",
    "                self.vectors[node] = np.append(vectors1[node], vectors2[node])\n",
    "        else:\n",
    "            self.vectors = self.model.get_embeddings()\n",
    "\n",
    "    def save_embeddings(self, filename):\n",
    "        fout = open(filename, 'w')\n",
    "        node_num = len(self.vectors.keys())\n",
    "        fout.write(\"{} {}\\n\".format(node_num, self.rep_size))\n",
    "        for node, vec in self.vectors.items():\n",
    "            fout.write(\"{} {}\\n\".format(node,\n",
    "                                        ' '.join([str(x) for x in vec])))\n",
    "        fout.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../BiNE-master/data/wiki/rating_train_wiki.dat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing homo graphs....\n",
      "The homo graph for users.....\n",
      "The homo graph for items.....\n"
     ]
    }
   ],
   "source": [
    "g = Graphs()\n",
    "g.read_edgelist(path,weighted=True)\n",
    "g.build_homo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17529"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.G.number_of_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43800"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.M.number_of_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-procesing for non-uniform negative sampling!\n",
      "The norm is 44117.771519484086\n",
      "The norm is 19349.29261155175\n",
      "The norm is 27507.586321488227\n",
      "The norm is 12222.21215052077\n",
      "Pre-procesing for non-uniform negative sampling!\n",
      "The norm is 44117.771519484086\n",
      "The norm is 19349.29261155175\n",
      "The norm is 27507.586321488227\n",
      "The norm is 12222.21215052077\n",
      "epoch:0 sum of loss:561.9734442532063\n",
      "epoch:0 sum of loss:406.1935770511627\n",
      "epoch:1 sum of loss:503.18235817551613\n",
      "epoch:1 sum of loss:282.75578133109957\n",
      "epoch:2 sum of loss:480.2685763128102\n",
      "epoch:2 sum of loss:244.70883455313742\n",
      "epoch:3 sum of loss:460.65317338658497\n",
      "epoch:3 sum of loss:231.0335335917771\n",
      "epoch:4 sum of loss:444.4130678907968\n",
      "epoch:4 sum of loss:222.81088832602836\n",
      "epoch:5 sum of loss:428.7160584559315\n",
      "epoch:5 sum of loss:216.5587756393943\n",
      "epoch:6 sum of loss:414.2424913667346\n",
      "epoch:6 sum of loss:210.39481715776492\n",
      "epoch:7 sum of loss:399.8439357805437\n",
      "epoch:7 sum of loss:203.8012097242754\n",
      "epoch:8 sum of loss:387.7898405694814\n",
      "epoch:8 sum of loss:196.23706839373335\n",
      "epoch:9 sum of loss:377.4509644458817\n",
      "epoch:9 sum of loss:187.98165839986177\n",
      "epoch:10 sum of loss:368.12266301101477\n",
      "epoch:10 sum of loss:179.83116906453506\n",
      "epoch:11 sum of loss:360.26627783916445\n",
      "epoch:11 sum of loss:171.65750836840016\n",
      "epoch:12 sum of loss:353.9107605710843\n",
      "epoch:12 sum of loss:163.82863759138854\n",
      "epoch:13 sum of loss:347.7456040238139\n",
      "epoch:13 sum of loss:156.74172070752684\n",
      "epoch:14 sum of loss:343.1480693923742\n",
      "epoch:14 sum of loss:150.30052297380462\n",
      "epoch:15 sum of loss:338.8285240419285\n",
      "epoch:15 sum of loss:144.4820205130527\n",
      "epoch:16 sum of loss:334.5960267961829\n",
      "epoch:16 sum of loss:139.43798343581147\n",
      "epoch:17 sum of loss:331.58057284210093\n",
      "epoch:17 sum of loss:134.8962036658404\n",
      "epoch:18 sum of loss:329.08337341403194\n",
      "epoch:18 sum of loss:130.60354984225478\n",
      "epoch:19 sum of loss:327.06230045173146\n",
      "epoch:19 sum of loss:127.10008242649837\n",
      "epoch:20 sum of loss:323.92724574855004\n",
      "epoch:20 sum of loss:124.0109510660318\n",
      "epoch:21 sum of loss:321.93608276666646\n",
      "epoch:21 sum of loss:121.5650102226299\n",
      "epoch:22 sum of loss:320.47280293162595\n",
      "epoch:22 sum of loss:118.76567121598976\n",
      "epoch:23 sum of loss:318.80506648788344\n",
      "epoch:23 sum of loss:117.04675552260835\n",
      "epoch:24 sum of loss:317.54003410606373\n",
      "epoch:24 sum of loss:114.98502717251137\n",
      "epoch:25 sum of loss:315.72055733577315\n",
      "epoch:25 sum of loss:113.08233827698365\n",
      "epoch:26 sum of loss:314.949913555766\n",
      "epoch:26 sum of loss:111.69595740841561\n",
      "epoch:27 sum of loss:314.18017645273466\n",
      "epoch:27 sum of loss:110.19824206933234\n",
      "epoch:28 sum of loss:312.84454346448547\n",
      "epoch:28 sum of loss:109.24244305583358\n",
      "epoch:29 sum of loss:311.39757382311024\n",
      "epoch:29 sum of loss:108.22340416156652\n",
      "epoch:30 sum of loss:311.42751945182704\n",
      "epoch:30 sum of loss:107.12622771052742\n",
      "epoch:31 sum of loss:310.6078834524378\n",
      "epoch:31 sum of loss:106.29248994825167\n",
      "epoch:32 sum of loss:309.62958392873406\n",
      "epoch:32 sum of loss:105.30437225339445\n",
      "epoch:33 sum of loss:309.3274590931833\n",
      "epoch:33 sum of loss:104.76439904485359\n",
      "epoch:34 sum of loss:307.9422167195007\n",
      "epoch:34 sum of loss:104.00437725766253\n",
      "epoch:35 sum of loss:307.7268139300868\n",
      "epoch:35 sum of loss:103.48043594560929\n",
      "epoch:36 sum of loss:307.01226193644106\n",
      "epoch:36 sum of loss:102.63191252558958\n",
      "epoch:37 sum of loss:306.6249287100509\n",
      "epoch:37 sum of loss:102.32485977328555\n",
      "epoch:38 sum of loss:305.8351032547653\n",
      "epoch:38 sum of loss:101.98791437299697\n",
      "epoch:39 sum of loss:305.41420927178115\n",
      "epoch:39 sum of loss:101.2988862538486\n",
      "epoch:40 sum of loss:304.7580559933558\n",
      "epoch:40 sum of loss:100.8219024939054\n",
      "epoch:41 sum of loss:304.884589465335\n",
      "epoch:41 sum of loss:100.61811854721773\n",
      "epoch:42 sum of loss:304.45508283842355\n",
      "epoch:42 sum of loss:99.919886694275\n",
      "epoch:43 sum of loss:304.30750600621104\n",
      "epoch:43 sum of loss:99.79935051990833\n",
      "epoch:44 sum of loss:303.65095623768866\n",
      "epoch:44 sum of loss:99.65139965498503\n",
      "epoch:45 sum of loss:303.4806025791913\n",
      "epoch:45 sum of loss:99.1317589535732\n",
      "epoch:46 sum of loss:302.6804382717237\n",
      "epoch:46 sum of loss:98.88010963251669\n",
      "epoch:47 sum of loss:301.778193670325\n",
      "epoch:47 sum of loss:98.45015862558292\n",
      "epoch:48 sum of loss:302.5422534989193\n",
      "epoch:48 sum of loss:98.66905024005584\n",
      "epoch:49 sum of loss:301.7174665015191\n",
      "epoch:49 sum of loss:98.09049028570618\n",
      "epoch:50 sum of loss:300.7782664876431\n",
      "epoch:50 sum of loss:97.86156285982032\n",
      "epoch:51 sum of loss:301.10936526954174\n",
      "epoch:51 sum of loss:97.61418340448796\n",
      "epoch:52 sum of loss:301.0707848118618\n",
      "epoch:52 sum of loss:97.45893012905451\n",
      "epoch:53 sum of loss:300.78762939432636\n",
      "epoch:53 sum of loss:97.45604142773948\n",
      "epoch:54 sum of loss:300.79845054633915\n",
      "epoch:54 sum of loss:97.08282902610914\n",
      "epoch:55 sum of loss:300.6720177405514\n",
      "epoch:55 sum of loss:96.54861747720705\n",
      "epoch:56 sum of loss:299.9524287208915\n",
      "epoch:56 sum of loss:96.66750820508817\n",
      "epoch:57 sum of loss:299.92305685859174\n",
      "epoch:57 sum of loss:96.38855275338604\n",
      "epoch:58 sum of loss:299.42969718249515\n",
      "epoch:58 sum of loss:96.4851122009675\n",
      "epoch:59 sum of loss:299.09568967018276\n",
      "epoch:59 sum of loss:96.46483828062281\n",
      "epoch:60 sum of loss:299.3429909902625\n",
      "epoch:60 sum of loss:96.19075909753543\n",
      "epoch:61 sum of loss:299.52776954555884\n",
      "epoch:61 sum of loss:95.91895794439634\n",
      "epoch:62 sum of loss:298.8930571535602\n",
      "epoch:62 sum of loss:95.65205756314569\n",
      "epoch:63 sum of loss:299.10286334808916\n",
      "epoch:63 sum of loss:95.98699804974503\n",
      "epoch:64 sum of loss:299.286637686193\n",
      "epoch:64 sum of loss:95.55839009733084\n",
      "epoch:65 sum of loss:298.16979840630665\n",
      "epoch:65 sum of loss:95.80697221415295\n",
      "epoch:66 sum of loss:298.80746004357934\n",
      "epoch:66 sum of loss:95.46511359823816\n",
      "epoch:67 sum of loss:297.81921878270805\n",
      "epoch:67 sum of loss:95.10273719180037\n",
      "epoch:68 sum of loss:298.33001730032265\n",
      "epoch:68 sum of loss:95.52885144157447\n",
      "epoch:69 sum of loss:298.6234356146306\n",
      "epoch:69 sum of loss:95.27597049103267\n",
      "epoch:70 sum of loss:298.1804117495194\n",
      "epoch:70 sum of loss:95.28922040395398\n",
      "epoch:71 sum of loss:297.7151882983744\n",
      "epoch:71 sum of loss:95.13058319674492\n",
      "epoch:72 sum of loss:297.6739654033445\n",
      "epoch:72 sum of loss:95.04946348352118\n",
      "epoch:73 sum of loss:298.4210575344041\n",
      "epoch:73 sum of loss:94.51208118017168\n",
      "epoch:74 sum of loss:297.1276850043796\n",
      "epoch:74 sum of loss:94.93092062889254\n",
      "epoch:75 sum of loss:296.9817687724717\n",
      "epoch:75 sum of loss:94.25797963412515\n",
      "epoch:76 sum of loss:297.1062419828959\n",
      "epoch:76 sum of loss:94.8031875338938\n",
      "epoch:77 sum of loss:296.5526262926869\n",
      "epoch:77 sum of loss:94.17744320769187\n",
      "epoch:78 sum of loss:296.72978739999235\n",
      "epoch:78 sum of loss:94.05518561529013\n",
      "epoch:79 sum of loss:296.2565397899598\n",
      "epoch:79 sum of loss:94.5606706636798\n",
      "epoch:80 sum of loss:297.0185489114374\n",
      "epoch:80 sum of loss:94.05923628073515\n",
      "epoch:81 sum of loss:295.7298082159832\n",
      "epoch:81 sum of loss:94.29920810854895\n",
      "epoch:82 sum of loss:296.44080474739894\n",
      "epoch:82 sum of loss:94.05541229187915\n",
      "epoch:83 sum of loss:295.7209736169316\n",
      "epoch:83 sum of loss:93.76546980033258\n",
      "epoch:84 sum of loss:296.1762371091172\n",
      "epoch:84 sum of loss:93.63042255877859\n",
      "epoch:85 sum of loss:295.9521188973449\n",
      "epoch:85 sum of loss:93.81774497272961\n",
      "epoch:86 sum of loss:295.99012759001926\n",
      "epoch:86 sum of loss:93.67105520060665\n",
      "epoch:87 sum of loss:296.73741983855143\n",
      "epoch:87 sum of loss:94.03744711270963\n",
      "epoch:88 sum of loss:295.7070542983711\n",
      "epoch:88 sum of loss:93.61652139206757\n",
      "epoch:89 sum of loss:295.95213415985927\n",
      "epoch:89 sum of loss:93.3167019212796\n",
      "epoch:90 sum of loss:295.86454677022994\n",
      "epoch:90 sum of loss:93.40781419967891\n",
      "epoch:91 sum of loss:295.7639713399112\n",
      "epoch:91 sum of loss:93.28253073312423\n",
      "epoch:92 sum of loss:296.3672693991102\n",
      "epoch:92 sum of loss:93.51826184630508\n",
      "epoch:93 sum of loss:294.30591663112864\n",
      "epoch:93 sum of loss:93.61135842858046\n",
      "epoch:94 sum of loss:295.02615641010925\n",
      "epoch:94 sum of loss:93.15683663510914\n",
      "epoch:95 sum of loss:295.0787970656529\n",
      "epoch:95 sum of loss:93.26418996641725\n",
      "epoch:96 sum of loss:295.65645842719823\n",
      "epoch:96 sum of loss:93.50489163114206\n",
      "epoch:97 sum of loss:294.9184468188323\n",
      "epoch:97 sum of loss:93.26393734227315\n",
      "epoch:98 sum of loss:294.9507031608373\n",
      "epoch:98 sum of loss:93.90401270236741\n",
      "epoch:99 sum of loss:296.01079165004194\n",
      "epoch:99 sum of loss:93.39761665569041\n"
     ]
    }
   ],
   "source": [
    "model = LINE(g, epoch = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_embeddings('vec_md.dat')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
