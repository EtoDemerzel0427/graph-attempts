{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huangweilan/anaconda3/lib/python3.6/site-packages/scipy/stats/morestats.py:12: DeprecationWarning: Importing from numpy.testing.decorators is deprecated, import from numpy.testing instead.\n",
      "  from numpy.testing.decorators import setastest\n"
     ]
    }
   ],
   "source": [
    "import mxnet as mx\n",
    "import mxnet.gluon\n",
    "from mxnet import nd, autograd\n",
    "import scipy.sparse as sp\n",
    "import networkx as nx\n",
    "from mxnet.gluon import Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing homo graphs....\n",
      "The homo graph for users.....\n",
      "The homo graph for items.....\n"
     ]
    }
   ],
   "source": [
    "from graph_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_to_tuple(sparse_mx):\n",
    "    '''\n",
    "    Convert sparse matrix to tuple representation.\n",
    "    \n",
    "    '''\n",
    "    def to_tuple(mx):\n",
    "        if not sp.isspmatrix_coo(mx):\n",
    "            mx = mx.tocoo()\n",
    "        coords = np.vstack((mx.row, mx.col)).transpose()\n",
    "        values = mx.data\n",
    "        shape = mx.shape\n",
    "        return coords, values, shape\n",
    "\n",
    "    if isinstance(sparse_mx, list):\n",
    "        for i in range(len(sparse_mx)):\n",
    "            sparse_mx[i] = to_tuple(sparse_mx[i])\n",
    "    else:\n",
    "        sparse_mx = to_tuple(sparse_mx)\n",
    "\n",
    "    return sparse_mx\n",
    "\n",
    "def normalize_adj(adj):\n",
    "    '''\n",
    "    Symmetrically normalize adjacency matrix.\n",
    "    norm_adj = D^{-0.5}*adj*D^{-0.5}.\n",
    "    \n",
    "    return the normalized adjcency matrix in coo format.\n",
    "    '''\n",
    "    adj = sp.coo_matrix(adj)\n",
    "    rowsum = np.array(adj.sum(1))\n",
    "    \n",
    "    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
    "    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ctx = mx.gpu()\n",
    "ctx = mx.gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.convert_node_labels_to_integers(gul.G) # the heter graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7178, 17699)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.number_of_nodes(), G.number_of_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = nx.disjoint_union(gul.M, gul.N) # the homo graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7178, 2920058)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H.number_of_nodes(), H.number_of_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj = nx.adjacency_matrix(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_dd = normalize_adj(adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_d = adj_dd.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "adj_hat = nd.sparse.csr_matrix(adj_d, ctx=ctx,dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 18240.0\n"
     ]
    }
   ],
   "source": [
    "print(np.min(adj.data), np.max(adj.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter those below threshold value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fout = open('new_homo_M.txt', 'w')\n",
    "\n",
    "fout.write('user item weight neg1 neg2 neg3 neg4 neg5\\n')\n",
    "\n",
    "with open('homo_M.txt') as fin:\n",
    "    next(fin)\n",
    "    for line in fin:\n",
    "        vec = line.strip().split(' ')\n",
    "        rating = float(vec[2])\n",
    "        if rating > 30:\n",
    "            fout.write(line)\n",
    "        \n",
    "\n",
    "fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fout = open('new_homo_N.txt', 'w')\n",
    "\n",
    "fout.write('user item weight neg1 neg2 neg3 neg4 neg5\\n')\n",
    "\n",
    "with open('homo_N.txt') as fin:\n",
    "    next(fin)\n",
    "    for line in fin:\n",
    "        vec = line.strip().split(' ')\n",
    "        rating = float(vec[2])\n",
    "        if rating > 5:\n",
    "            fout.write(line)\n",
    "        \n",
    "\n",
    "fout.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = H.number_of_nodes()\n",
    "rep_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding = nd.random.normal(loc=0, scale=0.01, shape = (num_nodes, rep_size), ctx=ctx)\n",
    "#embedding_out = nd.random.normal(loc=0, scale=0.01, shape = (num_nodes, rep_size), ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We do not apply dropout here\n",
    "# since it is an approach to avoid overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet.gluon import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class graph_conv(Block):\n",
    "    def __init__(self,  **kwargs):\n",
    "        super(graph_conv, self).__init__(**kwargs)\n",
    "        self.weight1 = self.params.get('weight1', shape=(rep_size, rep_size))\n",
    "      #  self.bias = self.params.get('bias', shape=(units,))\n",
    "        self.adj_hat = adj_hat\n",
    "        self.embedding = self.params.get('embedding', shape = (num_nodes, rep_size))\n",
    "        #self.weight2 = self.params.get('weight2', shape=(rep_size, rep_size))\n",
    "        \n",
    "    def forward(self):\n",
    "        x = nd.dot(self.embedding.data(), self.weight1.data())\n",
    "        hidden = nd.relu(nd.dot(self.adj_hat, x))\n",
    "       \n",
    "        return hidden\n",
    "\n",
    "        #y = nd.dot(hidden, self.weight2.data())\n",
    "        #return nd.relu(nd.dot(adj_hat, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#net3 = nn.Sequential()\n",
    "net1= graph_conv()\n",
    "net1.initialize(init=init.Xavier(rnd_type='gaussian'), ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[0.         0.00428166 0.         ... 0.00135739 0.         0.        ]\n",
       " [0.         0.001183   0.         ... 0.00259045 0.00067428 0.        ]\n",
       " [0.00152674 0.         0.         ... 0.00459627 0.00042417 0.        ]\n",
       " ...\n",
       " [0.         0.         0.         ... 0.00663018 0.         0.00519713]\n",
       " [0.00135232 0.         0.         ... 0.         0.00808461 0.01524705]\n",
       " [0.00750169 0.         0.00338491 ... 0.         0.01123093 0.        ]]\n",
       "<NDArray 7178x128 @gpu(0)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "net2 = graph_conv()\n",
    "net2.initialize(init=init.Xavier(rnd_type='gaussian'), ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[0.         0.         0.00189501 ... 0.00376245 0.00096783 0.        ]\n",
       " [0.         0.0006559  0.00307435 ... 0.00286185 0.00133105 0.00092116]\n",
       " [0.         0.0027033  0.00283865 ... 0.         0.00079506 0.00429375]\n",
       " ...\n",
       " [0.         0.00679323 0.         ... 0.00387856 0.01485468 0.        ]\n",
       " [0.         0.         0.         ... 0.01082747 0.         0.        ]\n",
       " [0.01528536 0.         0.00116018 ... 0.         0.         0.        ]]\n",
       "<NDArray 7178x128 @gpu(0)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "homo_n = pd.read_table('new_homo_N.txt', sep = ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "homo_m = pd.read_table('new_homo_M.txt', sep = ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "heter_u = pd.read_table('heter_u.txt', sep = ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "heter_v = pd.read_table('heter_i.txt', sep = ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_n = homo_n.shape[0]\n",
    "size_m = homo_m.shape[0]\n",
    "size_u = heter_u.shape[0]\n",
    "size_v = heter_v.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nodes = gul.node_u + gul.node_v\n",
    "look_up = dict(zip(all_nodes, np.arange(len(all_nodes))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_iter(df, batch_size = 300):\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    num_batch = len(df) // batch_size\n",
    "    for i in range(num_batch):\n",
    "        data = df.iloc[i*batch_size:(i+1)*batch_size]\n",
    "        h = data.iloc[:, 0].values\n",
    "        t = data.iloc[:, 1].values\n",
    "        neg = data.iloc[:, 3:].values\n",
    "        \n",
    "        h_ind = [[look_up[i]] for i in h]\n",
    "        t_ind = [[look_up[i]] for i in t]\n",
    "        neg_ind = []\n",
    "        for n in neg:\n",
    "            y = [look_up[i] for i in n]\n",
    "            neg_ind.append(y)\n",
    "            \n",
    "        yield h_ind, t_ind, neg_ind       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_first_loss(h,t,neg):\n",
    "    res_emb = net1()\n",
    "    \n",
    "    h_vec = res_emb.take(nd.array(h, ctx=ctx))\n",
    "    t_vec = res_emb.take(nd.array(t, ctx=ctx))\n",
    "    neg_vec = res_emb.take(nd.array(neg, ctx=ctx))\n",
    "    \n",
    "    loss_pos = -nd.sum(nd.log(nd.sigmoid(nd.sum(h_vec*t_vec,axis=2)))) \n",
    "    loss_neg = -nd.sum(nd.log(nd.sigmoid(-nd.sum(h_vec*neg_vec,axis=2))))\n",
    "    \n",
    "    loss = loss_pos + loss_neg\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_second_loss(h,t,neg):\n",
    "#    res_emb = net1(embedding)\n",
    " #   res_ctx = net2(embedding_out)\n",
    "    res_emb = net1()\n",
    "    res_ctx = net2()\n",
    "    h_vec = res_emb.take(nd.array(h, ctx=ctx))\n",
    "    t_vec = res_ctx.take(nd.array(t, ctx=ctx))\n",
    "    neg_vec = res_ctx.take(nd.array(neg, ctx=ctx))\n",
    "    loss_pos = -nd.sum(nd.log(nd.sigmoid(nd.sum(h_vec*t_vec,axis=2)))) \n",
    "    loss_neg = -nd.sum(nd.log(nd.sigmoid(-nd.sum(h_vec*neg_vec,axis=2))))\n",
    "    \n",
    "  #  loss = loss_pos + loss_neg\n",
    "  #  loss = loss_pos\n",
    "    loss = loss_pos + loss_neg\n",
    "    \n",
    "    return loss     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer1 = mx.gluon.Trainer(net1.collect_params(), 'adam', {'learning_rate': 0.015, 'wd': 0.0005})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer2 = mx.gluon.Trainer(net2.collect_params(), 'adam', {'learning_rate': 0.015, 'wd': 0.0005})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = int(0.1*size_m) + int(0.1*size_n) +  int(0.1*size_u) + int(0.1*size_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current loss:  \n",
      "[30588.71]\n",
      "<NDArray 1 @gpu(0)>\n",
      "current loss:  \n",
      "[30588.635]\n",
      "<NDArray 1 @gpu(0)>\n",
      "current loss:  \n",
      "[30588.594]\n",
      "<NDArray 1 @gpu(0)>\n",
      "current loss:  \n",
      "[30588.586]\n",
      "<NDArray 1 @gpu(0)>\n",
      "current loss:  \n",
      "[30588.586]\n",
      "<NDArray 1 @gpu(0)>\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epoch):\n",
    "    # Each epoch has 10 loops\n",
    "    batch_m = data_iter(homo_m, int(0.1*size_m))\n",
    "    batch_n = data_iter(homo_n, int(0.1*size_n))\n",
    "    batch_u = data_iter(heter_u, int(0.1*size_u))\n",
    "    batch_v = data_iter(heter_v, int(0.1*size_v))\n",
    "    \n",
    "    for batch in zip(batch_m, batch_n, batch_u, batch_v):\n",
    "        h1, t1, neg1 = batch[0]\n",
    "        h2, t2, neg2 = batch[1]\n",
    "        h3, t3, neg3 = batch[2]\n",
    "        h4, t4, neg4 = batch[3]\n",
    "        with autograd.record():\n",
    "            l1_u = compute_first_loss(h3, t3, neg3)\n",
    "            l1_v = compute_first_loss(h4, t4, neg4)\n",
    "            l2_m = compute_second_loss(h1, t1, neg1)\n",
    "            l2_n = compute_second_loss(h2, t2, neg2)\n",
    "            loss = l1_u + l1_v + l2_m + l2_n\n",
    "        loss.backward()\n",
    "        \n",
    "        trainer1.step(batch_size)\n",
    "        trainer2.step(batch_size)\n",
    "    print('current loss: ', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6001"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gul.node_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = net1.embedding.data().asnumpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save embeddings\n",
    "def save_embeddings(file1, file2):\n",
    "    fout_u = open(file1, 'w')\n",
    "    fout_v = open(file2, 'w')\n",
    "    \n",
    "    num_u = len(gul.node_u)\n",
    "    num_v = len(gul.node_v)\n",
    "    embedding = net1.embedding.data().asnumpy()\n",
    "    for u in range(num_u):\n",
    "        fout_u.write(\"{} {}\\n\".format(gul.node_u[u], ' '.join([str(x) for x in embedding[u]])))\n",
    "    \n",
    "    for i in range(num_u, num_u + num_v):\n",
    "        fout_v.write(\"{} {}\\n\".format(gul.node_v[i-num_u], ' '.join([str(x) for x in embedding[i]])))\n",
    "        \n",
    "    fout_u.close()\n",
    "    fout_v.close()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_embeddings('u.txt', 'v.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "net1.embedding.data().asnumpy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
